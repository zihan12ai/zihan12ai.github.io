<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/11/07/%E6%9C%AA%E5%91%BD%E5%90%8D/"/>
    <url>/2025/11/07/%E6%9C%AA%E5%91%BD%E5%90%8D/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/11/07/Dataset%20is%20all%20you%20need/"/>
    <url>/2025/11/07/Dataset%20is%20all%20you%20need/</url>
    
    <content type="html"><![CDATA[<p>[[æµ‹è¯•]]</p><h1 id="ç»™è‡ªå·±ç­”ç–‘"><a href="#ç»™è‡ªå·±ç­”ç–‘" class="headerlink" title="ç»™è‡ªå·±ç­”ç–‘"></a>ç»™è‡ªå·±ç­”ç–‘</h1><p>æˆ‘ä»¬ä¸åšé€šä¿¡ï¼Œ<strong>åªåšè®¡ç®—</strong>ï¼Œå¹¶ä¸”è¦åšåˆ°æè‡´ã€‚</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">æœ¬ç ”ç©¶ä¸“æ³¨äºLLMè®­ç»ƒä¸­çš„**å•èŠ‚ç‚¹è®¡ç®—æ€§èƒ½é¢„æµ‹ (Single-<span class="hljs-keyword">Node</span> <span class="hljs-title">Computational</span> Performance Prediction)**ã€‚æˆ‘ä»¬é€šè¿‡åœ¨å•GPUä¸Šæ¨¡æ‹Ÿä¸åŒå¹¶è¡Œç­–ç•¥ä¸‹çš„è®¡ç®—åˆ‡ç‰‡ï¼ˆcomputational sliceï¼‰ï¼Œæ¥ç²¾ç¡®æµ‹é‡å…¶è¿è¡Œæ—¶é—´ä¸å†…å­˜å¼€é”€ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ—¨åœ¨ä¸ºä¸Šå±‚åˆ†ææ¨¡å‹æä¾›ç²¾ç¡®çš„è®¡ç®—æˆæœ¬è¾“å…¥ï¼Œä»è€Œè§£è€¦è®¡ç®—ä¸é€šä¿¡çš„å¤æ‚æ€§ã€‚<br></code></pre></td></tr></table></figure><p>ä¸€ä¸ªå®Œæ•´çš„LLMè®­ç»ƒçš„<strong>è®¡ç®—è¿‡ç¨‹</strong>ï¼Œè¢«æ‹†åˆ†ä¸ºlayerã€submoduleã€kernelä¸‰ä¸ªçº§åˆ«ã€‚<br>åä¸¤ä¸ªæ˜¯è®¡ç®—å›¾èŠ‚ç‚¹çš„ä¸åŒç²’åº¦ã€‚</p><table><thead><tr><th align="center"></th><th align="center">ground truth</th><th align="center">proxy</th></tr></thead><tbody><tr><td align="center">layer(1,2,4,8â€¦â€¦)</td><td align="center">cuda eventï¼ˆçº¯ï¼‰â€æˆ‘æœ‰GPUï¼Œå¹¶ä¸”å®Œæ•´è®­ç»ƒè¿‡ï¼Œæ‰€ä»¥æˆ‘ç›´æ¥æŸ¥è¡¨â€œã€‚profilerï¼ˆoverheadï¼‰â€æˆ‘æƒ³çŸ¥é“ä¸€äº›è¯¦ç»†çš„æ•°æ®â€œ</td><td align="center"></td></tr><tr><td align="center">submoduleï¼ˆç”±aiobæ”¹ä¸ºtransformerè°ƒåŒ…ï¼‰</td><td align="center">Hookï¼ˆçº¯ï¼‰ã€profilerï¼ˆoverheadï¼‰</td><td align="center">Cuda eventâ€æˆ‘æœ‰GPUï¼Œä½†æ˜¯æˆ‘è¿è¡Œå®Œæ•´æ¨¡å‹å¤ªéº»çƒ¦äº†ï¼Œæˆ‘ç”¨ç¢ç‰‡æ—¶é—´æ¥æ‹¼å‡‘â€œã€‚é£é™©æ¯”è¾ƒå¤§ï¼Œè‡ªå†™çš„æ¨¡å—æ²¡æœ‰ç»è¿‡compileï¼Œåå‘ä¼ æ’­ä¹Ÿå¾ˆéš¾æµ‹</td></tr><tr><td align="center">kernel</td><td align="center">Profilerâ€æˆ‘ä¹°ä¸èµ·&#x2F;æ­£åœ¨è®¾è®¡GPUï¼Œæ‹¿ç®—å­æ•°æ®åº“æ—¶é—´æ¥æ‹¼å‡‘é¢„æµ‹â€œ</td><td align="center">Profiler</td></tr><tr><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">submoduleéƒ¨åˆ†æœ€å¥½ä½¿ç”¨ <code>torch.allclose(output_profiler, output_cuda event, atol=1e-5)</code> æ¥æ–­è¨€ä¸¤ä¸ªè¾“å‡ºåœ¨æ•°å€¼ä¸Šæ˜¯å¦è¶³å¤Ÿæ¥è¿‘ã€‚</td><td align="center"></td><td align="center"></td></tr><tr><td align="center">æ•°å€¼ï¼ˆallcloseï¼‰å’Œæ€§èƒ½ï¼ˆcompileå’Œeagerï¼‰éƒ½è¦éªŒè¯</td><td align="center"></td><td align="center"></td></tr></tbody></table><ul><li><code>model_aiob = AIOB_DecoderBlock(...)</code></li><li><code>model_hf = LlamaDecoderLayer(...)</code> (æ¥è‡ªHugging Face)</li></ul><img src="/2025/11/07/Dataset%20is%20all%20you%20need/test.png" class="" title="ceshi"><p>![[Pasted image 20251106145408.png]]</p><p>![[Pasted image 20251106150141.png]]</p><p><strong>å°† <code>torch.compile</code> ä½œä¸ºå®éªŒå˜é‡</strong>ã€‚â€åœ¨ä¸åŒæ¨¡å‹ã€ä¸åŒé…ç½®ã€ä¸åŒç¡¬ä»¶ä¸Šçš„æ”¶ç›Šç©¶ç«Ÿå¦‚ä½•ï¼Ÿâ€œ</p><h1 id="A-Survey-on-Performance-Modeling-and-Prediction-for-Distributed-DNN-Training"><a href="#A-Survey-on-Performance-Modeling-and-Prediction-for-Distributed-DNN-Training" class="headerlink" title="A Survey on Performance Modeling and Prediction  for Distributed DNN Training"></a>A Survey on Performance Modeling and Prediction  for Distributed DNN Training</h1><h2 id="2èƒŒæ™¯"><a href="#2èƒŒæ™¯" class="headerlink" title="2èƒŒæ™¯"></a>2èƒŒæ™¯</h2><p>è€ç”Ÿå¸¸è°ˆäº†ï¼Œä¸€ä¸ªå·¨å¤§è®­ç»ƒè¶…å‚æ•°çš„é…ç½®ç©ºé—´ï¼Œæ€ä¹ˆåœ¨æ­£å¼è¿è¡Œè®­ç»ƒè¿‡ç¨‹ä¹‹å‰æ‰¾ä¸€ä¸ªæœ€åˆé€‚çš„ã€‚çœé’±çœæ—¶é—´ï¼</p><h3 id="é…ç½®ç©ºé—´è¯¦ç»†ä»‹ç»"><a href="#é…ç½®ç©ºé—´è¯¦ç»†ä»‹ç»" class="headerlink" title="é…ç½®ç©ºé—´è¯¦ç»†ä»‹ç»"></a>é…ç½®ç©ºé—´è¯¦ç»†ä»‹ç»</h3><h4 id="1ã€å¹¶è¡Œç­–ç•¥"><a href="#1ã€å¹¶è¡Œç­–ç•¥" class="headerlink" title="1ã€å¹¶è¡Œç­–ç•¥"></a>1ã€å¹¶è¡Œç­–ç•¥</h4><h5 id="æ•°æ®å¹¶è¡Œ"><a href="#æ•°æ®å¹¶è¡Œ" class="headerlink" title="æ•°æ®å¹¶è¡Œ"></a>æ•°æ®å¹¶è¡Œ</h5><p>æ•°æ®é›†å¤šçš„æ—¶å€™ä¼šç”¨è¿™ä¸ª</p><h5 id="æ¨¡å‹å¹¶è¡Œï¼ˆæµæ°´çº¿ã€å¼ é‡ã€MoEçš„EPï¼‰"><a href="#æ¨¡å‹å¹¶è¡Œï¼ˆæµæ°´çº¿ã€å¼ é‡ã€MoEçš„EPï¼‰" class="headerlink" title="æ¨¡å‹å¹¶è¡Œï¼ˆæµæ°´çº¿ã€å¼ é‡ã€MoEçš„EPï¼‰"></a>æ¨¡å‹å¹¶è¡Œï¼ˆæµæ°´çº¿ã€å¼ é‡ã€MoEçš„EPï¼‰</h5><p>æ¨¡å‹æ”¾ä¸ä¸‹ç”¨è¿™ä¸ª</p><h5 id="æ··åˆå¹¶è¡Œ"><a href="#æ··åˆå¹¶è¡Œ" class="headerlink" title="æ··åˆå¹¶è¡Œ"></a>æ··åˆå¹¶è¡Œ</h5><p>æœ‰ä¸€äº›è®ºæ–‡è®¨è®ºè¿‡æ€ä¹ˆè‡ªåŠ¨æ‰¾ç­–ç•¥</p><h4 id="2ã€è®¡ç®—ä¼˜åŒ–"><a href="#2ã€è®¡ç®—ä¼˜åŒ–" class="headerlink" title="2ã€è®¡ç®—ä¼˜åŒ–"></a>2ã€è®¡ç®—ä¼˜åŒ–</h4><p>ä¸€æ–¹é¢æ˜¯ä¼˜åŒ–è®¡ç®—ä»»åŠ¡çš„åˆ†é…ï¼ˆèŠ‚ç‚¹ä¹‹é—´ï¼‰<br>ä¸€æ–¹é¢æ˜¯è®¾ç½®ã€ç®—å­ä¼˜åŒ–ï¼ˆèŠ‚ç‚¹ä¹‹å†…ï¼‰</p><h4 id="3ã€é€šä¿¡ä¼˜åŒ–"><a href="#3ã€é€šä¿¡ä¼˜åŒ–" class="headerlink" title="3ã€é€šä¿¡ä¼˜åŒ–"></a>3ã€é€šä¿¡ä¼˜åŒ–</h4><p>ç•¥</p><h4 id="4ã€å†…å­˜å’Œæ•°æ®åŠ è½½"><a href="#4ã€å†…å­˜å’Œæ•°æ®åŠ è½½" class="headerlink" title="4ã€å†…å­˜å’Œæ•°æ®åŠ è½½"></a>4ã€å†…å­˜å’Œæ•°æ®åŠ è½½</h4><p>å†…å­˜æ–¹é¢ï¼Œå¯ä»¥è®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚æ¯”å¦‚ZeROå®¶æ—<br>åŠ è½½æ–¹é¢ï¼ŒI&#x2F;Oä¸€å®šä¸èƒ½é™åˆ¶è®­ç»ƒé€Ÿåº¦ï¼ğŸ«²ğŸ˜­ğŸ«±</p><h4 id="5ã€é›†ç¾¤è®¾è®¡"><a href="#5ã€é›†ç¾¤è®¾è®¡" class="headerlink" title="5ã€é›†ç¾¤è®¾è®¡"></a>5ã€é›†ç¾¤è®¾è®¡</h4><p>æ‹“æ‰‘æ€ä¹ˆè®¾è®¡ã€ç½‘ç»œä¼ è¾“åè®®<br>ï¼ˆæ€ä¹ˆé€‰å¡ï¼Ÿè¿™ä¸ªæ–‡ç« é‡Œé¢æ²¡æåˆ°ï¼‰</p><h4 id="6ã€ååŒè®¾è®¡"><a href="#6ã€ååŒè®¾è®¡" class="headerlink" title="6ã€ååŒè®¾è®¡"></a>6ã€ååŒè®¾è®¡</h4><p>ä¸Šé¢è¿™äº›å†…å®¹ä¼šäº’ç›¸å½±å“ï¼Œæœ‰è€¦åˆã€‚æ¯”å¦‚è®¡ç®—å’Œé€šä¿¡å°±ä¼šé‡ã€‚<br>å¾ˆéš¾</p><h2 id="3æ­£æ–‡"><a href="#3æ­£æ–‡" class="headerlink" title="3æ­£æ–‡"></a>3æ­£æ–‡</h2><p>æŠŠæ¨¡å‹åˆ†ä¸ºä¸‰ç§ï¼š</p><ul><li>åˆ†ææ¨¡å‹ï¼šå°±æ˜¯ç”¨å…¬å¼ã€‚åˆ†è§£ä¸ºè®¡ç®—å’Œé€šä¿¡ä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«ç”¨åˆ†ææ¨¡å‹æ¥ä¼°è®¡æ—¶é—´ã€‚</li><li>åŸºäºå›¾ï¼š</li><li>æ‰§è¡Œé©±åŠ¨ï¼šä»¿çœŸæ¨¡æ‹Ÿã€‚</li></ul><h1 id="A-Survey-of-End-to-End-Modeling-for-Distributed-DNN-Training-Workloads-Simulators-and-TCO"><a href="#A-Survey-of-End-to-End-Modeling-for-Distributed-DNN-Training-Workloads-Simulators-and-TCO" class="headerlink" title="A Survey of End-to-End Modeling for Distributed  DNN Training: Workloads, Simulators, and TCO"></a>A Survey of End-to-End Modeling for Distributed  DNN Training: Workloads, Simulators, and TCO</h1><h2 id="2å·¥ä½œè´Ÿè½½è¡¨ç¤º"><a href="#2å·¥ä½œè´Ÿè½½è¡¨ç¤º" class="headerlink" title="2å·¥ä½œè´Ÿè½½è¡¨ç¤º"></a>2å·¥ä½œè´Ÿè½½è¡¨ç¤º</h2><h3 id="Bå·¥ä½œè´Ÿè½½è¡¨ç¤º"><a href="#Bå·¥ä½œè´Ÿè½½è¡¨ç¤º" class="headerlink" title="Bå·¥ä½œè´Ÿè½½è¡¨ç¤º"></a>Bå·¥ä½œè´Ÿè½½è¡¨ç¤º</h3><p>ä¸¤ç±»ï¼šé…ç½®ã€ä¸­é—´è¡¨ç¤º<br>çœ‹å›¾3éå¸¸æ˜“æ‡‚ï¼Œè¿˜åˆ†ä¸ºæ¡†æ¶é€šç”¨å’Œä¸“ç”¨ï¼Œå¸¦æ ‡è®°çš„å¹¶ä¸æ˜¯ä»é›¶å¼€å§‹å‘æ˜çš„å…¨æ–°IR</p><p>åŸºäºé…ç½®ï¼Œä¸å¤Ÿç»†èŠ‚ï¼Œä¾¿äºç”¨æˆ·å®šä¹‰ä½†ä¸ä¾¿äºä½œä¸ºtrace</p><h3 id="Cå¯å‘ä¸å±•æœ›"><a href="#Cå¯å‘ä¸å±•æœ›" class="headerlink" title="Cå¯å‘ä¸å±•æœ›"></a>Cå¯å‘ä¸å±•æœ›</h3><p>è®¡ç®—&#x2F;é€šä¿¡ä¹‹å¤–ä¹Ÿéœ€è¦å»ºæ¨¡ï¼Œæ¯”å¦‚I&#x2F;Oå’Œcheckpoint<br>è™½ç„¶æœ‰å¾ˆå¤šä¸­é—´è¡¨ç¤ºæ–¹æ³•ï¼Œä½†å®é™…ä¸Šå¾ˆå¤šæ¨¡æ‹Ÿå™¨æœ‰è‡ªå·±ç‰¹æœ‰çš„IRï¼Œä¾¿äºåŠŸèƒ½é›†æˆ<br>torch.fxæ˜¯æœ€å¸¸ç”¨çš„é€‰æ‹©<br>MLIRåœ¨ç¼–è¯‘å™¨ä¼˜åŒ–ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½†æ˜¯æ¨¡æ‹Ÿå™¨ä¸­ç”¨å¾—å¾ˆå°‘<br>æœºå™¨çº§è¡¨ç¤ºä¸å¸¸è§ï¼Œå¯èƒ½æ˜¯å› ä¸ºç²’åº¦å¤ªå°æ¨¡æ‹Ÿè´¹åŠ²</p><h2 id="3æ¨¡æ‹Ÿå™¨"><a href="#3æ¨¡æ‹Ÿå™¨" class="headerlink" title="3æ¨¡æ‹Ÿå™¨"></a>3æ¨¡æ‹Ÿå™¨</h2><h3 id="AèƒŒæ™¯å’ŒåŠ¨æœº"><a href="#AèƒŒæ™¯å’ŒåŠ¨æœº" class="headerlink" title="AèƒŒæ™¯å’ŒåŠ¨æœº"></a>AèƒŒæ™¯å’ŒåŠ¨æœº</h3><p>ä¿çœŸåº¦å’Œé€Ÿåº¦å¦‚ä½•æƒè¡¡<br>é€šç”¨çš„æ¡†æ¶åŸºæœ¬ä¸Šå°±æ˜¯å›¾4ï¼Œ</p><h3 id="Bæ¨¡æ‹Ÿå™¨åŸºæœ¬æ¶æ„çš„ç ”ç©¶"><a href="#Bæ¨¡æ‹Ÿå™¨åŸºæœ¬æ¶æ„çš„ç ”ç©¶" class="headerlink" title="Bæ¨¡æ‹Ÿå™¨åŸºæœ¬æ¶æ„çš„ç ”ç©¶"></a>Bæ¨¡æ‹Ÿå™¨åŸºæœ¬æ¶æ„çš„ç ”ç©¶</h3><h3 id="Cæ¨¡æ‹Ÿå™¨åˆ†ç±»ä¸æ¯”è¾ƒ"><a href="#Cæ¨¡æ‹Ÿå™¨åˆ†ç±»ä¸æ¯”è¾ƒ" class="headerlink" title="Cæ¨¡æ‹Ÿå™¨åˆ†ç±»ä¸æ¯”è¾ƒ"></a>Cæ¨¡æ‹Ÿå™¨åˆ†ç±»ä¸æ¯”è¾ƒ</h3><p>æ¨¡å‹åˆ†ä¸ºä¸‰ç§ï¼Œå¦‚å›¾5</p><ul><li>Analyticalï¼šçº¯æ•°å­¦åˆ†æï¼Œå¿«é€Ÿä½†ç²¾åº¦æœ‰é™ã€‚å·¥ä½œè´Ÿè½½çš„ç²’åº¦åŒºåˆ«å¸¦æ¥ç²¾ç¡®åº¦çš„å·®å¼‚ã€‚</li><li>Profilingï¼šç»“åˆäº†å®è¯æ•°æ®çš„Analyticalæ¨¡å‹ï¼Œé€Ÿåº¦å’Œç²¾åº¦å¹³è¡¡ã€‚è¦æ³¨æ„profilingçš„æˆæœ¬é—®é¢˜ï¼Œè¦ä¸ç„¶æ¨¡æ‹Ÿå’ŒçœŸå®æ‰§è¡Œæ—¶é—´æ²¡åŒºåˆ«ï¼Œå°±å¤±å»äº†æ¨¡æ‹Ÿçš„æ„ä¹‰ã€‚</li><li>Executionï¼šç²¾åº¦å¾ˆé«˜ä½†å¾ˆæ…¢</li></ul><p>åœ¨è¿™ä¸ªåŸºç¡€ä¸Šè¿˜å¯¹æ¨¡æ‹Ÿå™¨çš„ç²’åº¦è¿›è¡Œäº†åŒºåˆ†ï¼Œè§å›¾5ã€‚è¶Šç»†ç²’åº¦è¶Šç²¾ç¡®è¶Šæ…¢<br>è¡¨6æ˜¯è¿™ä¸ªæ–‡ç« çš„æ ¸å¿ƒï¼ŒLLMCompasså’ŒLLMServingSimæ˜¯åšæ¨ç†çš„ä½†æ˜¯è§’åº¦å¾ˆæ–°é¢–ã€‚<br>åŸºæœ¬ä¸Šéƒ½æ˜¯è®¡ç®—å’Œé€šä¿¡åˆ†å¼€å»ºæ¨¡ï¼Œç„¶ååè°ƒoverlapã€‚</p><h3 id="Då¯å‘ä¸å±•æœ›"><a href="#Då¯å‘ä¸å±•æœ›" class="headerlink" title="Då¯å‘ä¸å±•æœ›"></a>Då¯å‘ä¸å±•æœ›</h3><ol><li>å·¥ä½œè´Ÿè½½LLMé©±åŠ¨ï¼Œç¼ºä¹é€šç”¨æ€§</li><li>åŸºäºé…ç½®çš„å·¥ä½œè´Ÿè½½æ­£åœ¨è¢«æ·˜æ±°</li><li>99%æ˜¯è‹±ä¼Ÿè¾¾çš„ç¡¬ä»¶ï¼Œèƒ½ä¸èƒ½æ¨å¹¿å‘¢</li><li>éªŒè¯æ˜¯ä¸€ä¸ªå¤§å¼±ç‚¹ï¼Œæ²¡åŠæ³•çœŸçš„æ‹¿å‡ åƒå¼ å¡æ¥è·‘ground truth</li><li>ç½‘ç»œæ¨¡å‹éƒ½å¤ªç®€å•äº†ï¼Œå¿½ç•¥äº†æ‹¥å¡å’Œæ‹“æ‰‘</li><li>ASTRA-simæ˜¯ä¸€ä¸ªå¾ˆæœ‰å½±å“åŠ›çš„â€æ¨¡å—åŒ–â€œæ¡†æ¶</li><li>Pythonè´Ÿè´£ä¸Šå±‚ï¼ŒC++è´Ÿè´£åº•å±‚ç»†èŠ‚</li><li>æ¨¡æ‹Ÿå™¨æ™®éç¼ºä¹â€œèƒ½è€—â€æ¨¡å‹</li><li>æ¨¡æ‹Ÿå™¨å¿½è§†äº†â€œç¡¬ä»¶è´­ä¹°æˆæœ¬â€ï¼ˆèµ„æœ¬æˆæœ¬ï¼‰</li><li>å†…å­˜ï¼ˆæœ¬åœ°å’Œè¿œç«¯ï¼‰è¢«ä¸¥é‡å¿½è§†ï¼Œå»ºæ¨¡éå¸¸ç²—ç³™</li></ol><h1 id="è¡¨æ ¼"><a href="#è¡¨æ ¼" class="headerlink" title="è¡¨æ ¼"></a>è¡¨æ ¼</h1><table><thead><tr><th align="center">æ˜¯å¦é€‰ä¸­</th><th align="center">çŠ¶æ€</th><th align="center">The work</th><th align="center">å¼•ç”¨æ¬¡æ•°</th><th align="center">æ¥æº</th><th align="center">è®¡ç®—æ”¶é›†</th><th align="center">é¢„æµ‹æ–¹æ³•</th><th align="center">æˆ‘ä»¬è¿˜èƒ½åšä»€ä¹ˆ</th></tr></thead><tbody><tr><td align="center"></td><td align="center">è¯»å®Œï¼Œä¸»è¦åšçš„æ˜¯CVï¼Œå¤ç°å¾…å®š</td><td align="center">Modeling the Training Iteration Time for Heterogeneous Distributed Deep Learning Systems[69] CåˆŠIJIS 2023</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[138]</td><td align="center">ä¸€æ¬¡è¿­ä»£çš„å¢™ä¸Šæ—¶é—´</td><td align="center">æ ¸å¿ƒæ˜¯æ¨å¯¼FLOPs&#x2F;FLOPSï¼Œä¹‹åç”¨æ³°å‹’ç»éªŒæ ¡å‡†</td><td align="center">ä½œä¸ºä¸€ä¸ªéå¸¸åŸºç¡€çš„baseline</td></tr><tr><td align="center"></td><td align="center">è¯»å®Œï¼Œä¸»æ‰“æ˜¯transformerï¼Œè·‘é€šç¤ºä¾‹</td><td align="center">[[AMPeD]]:An Analytical Model for Performance in  Distributed Training of Transformers[72] Cä¼šISPASS 2023</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[75]; arxiv-sim</td><td align="center"></td><td align="center">æ ¸å¿ƒæ˜¯ç”¨GPUçš„MACï¼ˆMultiply-Accumulateï¼‰ï¼Œä¹‹åä¹Ÿè¦æ ¡å‡†</td><td align="center">ä½œä¸ºä¸€ä¸ªéå¸¸åŸºç¡€çš„baseline</td></tr><tr><td align="center"></td><td align="center">æœ‰ç‚¹è€ä¸ç¡®å®šçœ‹ä¸çœ‹</td><td align="center">Optimus:Â an efficient dynamic resource scheduler for deep learning clusters[73] Aä¼š2018</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[85]; arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center">çƒ‚åˆŠä¸çœ‹</td><td align="center">Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs[74]</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center">çƒ‚åˆŠä¸çœ‹</td><td align="center">Performance analysis of distributed deep learning frameworks in a multi-GPU environment[75]</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center">è¯»å®Œï¼Œä¸å¼€æºï¼Œå¤ç°å¾…å®š</td><td align="center">Predicting throughput of distributed stochastic gradient descent[76]AåˆŠ2022</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[62]</td><td align="center"></td><td align="center">è®¡ç®—æ—¶é—´æ˜¯å·²çŸ¥è¾“å…¥</td><td align="center">æ•°æ®é›†ä½œä¸ºå‚è€ƒè¡¥å……</td></tr><tr><td align="center"></td><td align="center">çƒ‚åˆŠä¸çœ‹</td><td align="center">SMSG: Profiling-Free Parallelism Modeling for Distributed Training of DNN[77]</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[123]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">[[FasterMoE]]: Modeling and Optimizing Training of  Large-Scale Dynamic Pre-Trained Models[78] Aä¼šPPoPP 2022</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[31]</td><td align="center"></td><td align="center">çº¯å…¬å¼è®¡ç®—</td><td align="center"></td></tr><tr><td align="center"></td><td align="center">è¯»å®Œï¼Œé’ˆå¯¹LLMåšçš„å·¥ä½œï¼Œè·‘é€šç¤ºä¾‹</td><td align="center">[[Calculon]]: A methodology and tool for high-level codesign of systems and large language models[80]Aä¼šSC 2023</td><td align="center"></td><td align="center">TPD A.åˆ†ææ¨¡å‹ è¡¨1ä¸º[38]; arxiv-sim</td><td align="center"></td><td align="center">FLOPSç†è®ºè®¡ç®—</td><td align="center">baseline</td></tr><tr><td align="center"></td><td align="center"></td><td align="center">[[Paleo]] å¥½ä¼šICLRä½†ä¸æ˜¯ccfæ¨è2017ï¼Œå¤ªç»å…¸äº†å¿…é¡»çœ‹</td><td align="center"></td><td align="center">TPB B.åŸºäºå›¾ è¡¨2ä¸º[89]; arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center">bert</td><td align="center">Beyond data and model parallelism for deep neural networks[93]å¥½ä¼šmlsysä½†ä¸æ˜¯ccfæ¨è2019</td><td align="center"></td><td align="center">TPD B.åŸºäºå›¾ è¡¨2ä¸º[43]; arxiv-sim;</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Daydream: Accurately estimating the efficacy of optimizations for DNN training[95]Aä¼šATC 2020</td><td align="center"></td><td align="center">TPD B.åŸºäºå›¾ è¡¨2ä¸º[146]; arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">dPRO: A generic performance diagnosis and optimization toolkit for expediting distributed DNN training[98]å¥½ä¼šmlsysä½†ä¸æ˜¯ccfæ¨è2022</td><td align="center"></td><td align="center">TPD B.åŸºäºå›¾ è¡¨2ä¸º[36]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center">LLM</td><td align="center">DistSim: A performance model of large-scale hybrid distributed DNN training[99]Cä¼šCF 2023</td><td align="center"></td><td align="center">TPD B.åŸºäºå›¾ è¡¨2ä¸º[69]; arxiv-sim</td><td align="center"></td><td align="center">è®¡ç®—ä½œä¸ºå·²çŸ¥åµŒå…¥</td><td align="center"></td></tr><tr><td align="center"></td><td align="center">LLM</td><td align="center">Proteus: Simulating the performance of distributed DNN training[102]AåˆŠTPDS2024</td><td align="center"></td><td align="center">TPD B.åŸºäºå›¾ è¡¨2ä¸º[20]</td><td align="center">ç”¨ä¸€ä¸ªåˆ†æå™¨ç›´æ¥æµ‹é‡</td><td align="center">è®¡ç®—ä½œä¸ºå·²çŸ¥åµŒå…¥</td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">(TAG)Expediting distributed DNN training with device topology-aware graph deployment[104]AåˆŠTPDS 2023</td><td align="center"></td><td align="center">TPD B.åŸºäºå›¾ è¡¨2ä¸º[140]</td><td align="center"></td><td align="center">è®¡ç®—ä½œä¸ºå·²çŸ¥åµŒå…¥</td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">TPD C.æ‰§è¡Œé©±åŠ¨ è¡¨3ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">TPD C.æ‰§è¡Œé©±åŠ¨ è¡¨3ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">TPD C.æ‰§è¡Œé©±åŠ¨ è¡¨3ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">PerfEstimator</td><td align="center"></td><td align="center">TPD C.æ‰§è¡Œé©±åŠ¨ è¡¨3ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">DNNEmu</td><td align="center"></td><td align="center">TPD C.æ‰§è¡Œé©±åŠ¨ è¡¨3ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">DistIR</td><td align="center"></td><td align="center">TPD C.æ‰§è¡Œé©±åŠ¨ è¡¨3ä¸º[]</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">ASTRA-Sim ä¸¤ç‰ˆæœ¬éƒ½æ˜¯Cä¼šISPASS 2020&#x2F;2023</td><td align="center"></td><td align="center">TPD C.æ‰§è¡Œé©±åŠ¨; SimAI; arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Chakra: Advancing Performance Benchmarking and Co-design using Standardized Execution Traces[115] æœªå‘è¡¨ 2023</td><td align="center"></td><td align="center">arxiv-IR</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Distir: An intermediate representation for optimizing distributed neural networks.[106] æœªå‘è¡¨ 2023</td><td align="center"></td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">SimAI[124]</td><td align="center"></td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">(multiverse)Accelerating Design Space Exploration for LLM Training Systems with Multi-experiment Parallel Simulation[43]</td><td align="center"></td><td align="center">arxiv-sim</td><td align="center"></td><td align="center">é‡ç‚¹ä¸ºé€šä¿¡ï¼Œç›´æ¥å‡è®¾è®¡ç®—æ—¶é—´å·²çŸ¥ã€‚ä½¿ç”¨chakraå·¥å…·ç”Ÿæˆè®¡ç®—æ—¶é—´ï¼Œå®é™…ä¸Šè¿™ä¸ªä¸œè¥¿è·Ÿpytorchå®˜æ–¹çš„kinetoæœ‰å¾ˆå¤§é‡å ã€‚æˆ‘ä»¬ç›´æ¥ç”¨kinetoï¼ˆprofilerï¼‰å°±è¡Œã€‚</td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">(Neusight)Forecasting GPU Performance for Deep Learning  Training and Inference[77]Aä¼šASPLOS</td><td align="center">15</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">ATLAHS: An Application-centric Network Simulator Toolchain for AI, HPC, and Distributed Storage[109]æœªå‘è¡¨</td><td align="center">1</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Echo: Simulating distributed training at scale.[35]æœªå‘è¡¨</td><td align="center"></td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Towards Universal Performance Modeling for  Machine Learning Training on  Multi-GPU Platforms[79]AåˆŠTPDS 2024</td><td align="center">4</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">vTrain: A Simulation Framework for Evaluating Cost-effective  and Compute-optimal Large Language Model Training AåˆŠMICRO2024</td><td align="center">24</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">LLMServingSim: A HW&#x2F;SW Co-Simulation  Infrastructure for LLM Inference Serving at Scaleçƒ‚åˆŠ</td><td align="center">19</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Proteus: Simulating the performance of distributed dnn training.AåˆŠTPDS2024</td><td align="center">13</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Llmem: Estimating gpu memory usage for fine-tuning pre-trained llms.æœªå‘è¡¨</td><td align="center">16</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">âˆš</td><td align="center"></td><td align="center">LLMCompass: Enabling Efficient Hardware Design  for Large Language Model Inference Aä¼šISCA2024</td><td align="center">45</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center">Deepflow: A cross-stack pathfinding framework for distributed ai systems.[6]BåˆŠTODAES 2024</td><td align="center">20</td><td align="center">arxiv-sim</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">âˆš</td><td align="center"></td><td align="center">(DNNMem)Estimating GPU Memory Consumption of Deep Learning Models Aä¼šFSE2020</td><td align="center">186</td><td align="center">arxiv-sim; å¸ˆå§</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>æµ‹è¯•</title>
    <link href="/2025/11/03/%E6%B5%8B%E8%AF%95/"/>
    <url>/2025/11/03/%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<img src="/2025/11/03/%E6%B5%8B%E8%AF%95/test.png" class="" title="å›¾ç‰‡å¼•ç”¨æ–¹æ³•ä¸€"> <p><img src="/test.png" alt="å›¾ç‰‡å¼•ç”¨æ–¹æ³•äºŒ"></p><p> <img src="/images/test.png" alt="å›¾ç‰‡å¼•ç”¨æ–¹æ³•ä¸‰"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/11/02/hello-world/"/>
    <url>/2025/11/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
